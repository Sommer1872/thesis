{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main thesis regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make sure you're in the right directory\n",
    "%cd \"/Users/simon/code/thesis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "\n",
    "import linearmodels\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly_express as px\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import anderson_ksamp\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from combine_stats_and_frag.load_daily_data import load_frag_data, load_market_quality_statistics, load_copustat\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragmentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frag = load_frag_data()\n",
    "\n",
    "# filter\n",
    "print(frag.shape)\n",
    "print(\"First date: \\t\", frag.index.min())\n",
    "print(\"Last date: \\t\", frag.index.max())\n",
    "\n",
    "frag.set_index(\"isin\", append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compustat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compustat = load_copustat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market quality data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"20200129_09-02-47_liquidity_stats.csv\"\n",
    "filepath = Path(f\"statistics/daily_liquidity/{filename}\")\n",
    "assert filepath.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load stats\n",
    "daily_stats = load_market_quality_statistics(filepath=filepath)\n",
    "\n",
    "# append \"isin\" to index\n",
    "daily_stats.set_index(\"isin\", append=True, inplace=True)\n",
    "\n",
    "print(daily_stats.shape)\n",
    "print(\"First date: \\t\", daily_stats.index.get_level_values(\"date\").min())\n",
    "print(\"Last date: \\t\", daily_stats.index.get_level_values(\"date\").max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_stats.rename(columns={\"num_transactions\": \"num_orders_aggr\"}, inplace=True)\n",
    "daily_stats.rename(columns={\"num_orders_total\": \"num_orders_passive\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_stats[\"quoted_rel_spread_bps_time_weighted\"] *= 100\n",
    "daily_stats[\"eff_rel_spread_bps_weighted\"] *= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the three dataframes into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine\n",
    "stats = daily_stats.join(frag, how=\"left\", lsuffix=\"_IMI\", sort=False)\n",
    "stats = stats.join(compustat, how=\"left\", rsuffix=\"_compu\", sort=False)\n",
    "\n",
    "# first level of index needs to be entity variable\n",
    "stats = stats.reset_index(\"date\").set_index(\"date\", append=True)\n",
    "\n",
    "print(\"First date: \\t\", stats.index.get_level_values(\"date\").min())\n",
    "print(\"Last date: \\t\", stats.index.get_level_values(\"date\").max())\n",
    "print(stats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create quartiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition = stats.index.get_level_values(\"date\") < pd.Timestamp(\"2019-07-01\")\n",
    "turnover_stats = stats[\"turnover\"].reset_index(\"isin\").groupby(\"isin\").median()\n",
    "\n",
    "lower_quartile = turnover_stats[\"turnover\"].quantile(0.25)\n",
    "median = turnover_stats[\"turnover\"].median()\n",
    "upper_quartile = turnover_stats[\"turnover\"].quantile(0.75)\n",
    "\n",
    "conditions = {\"3 bottom turnover\": turnover_stats[\"turnover\"] < lower_quartile,\n",
    "              \"2 low turnover\": (lower_quartile <= turnover_stats[\"turnover\"]) & (turnover_stats[\"turnover\"] < median),\n",
    "              \"1 high turnover\": (median <= turnover_stats[\"turnover\"]) & (turnover_stats[\"turnover\"] < upper_quartile),\n",
    "              \"0 top turnover\": upper_quartile <= turnover_stats[\"turnover\"]\n",
    "             }\n",
    "\n",
    "stats.reset_index(\"date\", inplace=True)\n",
    "\n",
    "for quartile, condition in conditions.items():\n",
    "    isins = turnover_stats[condition].index\n",
    "    stats.loc[isins, \"turnover_category\"] = quartile \n",
    "    \n",
    "stats.set_index(\"date\", append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stocks = stats[\"turnover_category\"].reset_index().groupby(\"turnover_category\")[\"isin\"].nunique()\n",
    "print(f\"Total number of stocks {num_stocks.sum()}\")\n",
    "num_stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding low turnover stocks?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude bottom turnover from sample?\n",
    "stats = stats[~stats[\"turnover_category\"].isin([\"3 bottom turnover\", \"2 low turnover\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stocks = stats[\"turnover_category\"].reset_index().groupby(\"turnover_category\")[\"isin\"].nunique()\n",
    "print(f\"Total number of stocks {num_stocks.sum()}\")\n",
    "num_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_isins = stats.index.get_level_values(\"isin\").unique()\n",
    "relevant_isins = relevant_isins.to_frame().reset_index(drop=True)\n",
    "# # Export isins to csv?\n",
    "# relevant_isins.to_csv(\"relevant_isins.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market share quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_measure = \"market_share\"  # \"non_fragmentation_index\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag_per_isin = stats.groupby([\"after_nonequivalence\", \"isin\"])[frag_measure].quantile(0.5)\n",
    "frag_per_isin = frag_per_isin.unstack(\"after_nonequivalence\")\n",
    "frag_per_isin[frag_measure] = frag_per_isin[True] - frag_per_isin[False]\n",
    "frag_per_isin.drop(columns=[False, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "condition = stats.index.get_level_values(\"date\") < pd.Timestamp(\"2019-07-01\")\n",
    "frag_per_isin = stats.loc[condition, [frag_measure]].reset_index(\"isin\")\n",
    "frag_per_isin = frag_per_isin.groupby([\"isin\"]).quantile(0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Option 1: simple \n",
    "# # a stock is not fragmented, if on more than 50% of all trading days, there was no trading on other venues (see cell above)\n",
    "# nonfragmentation = frag_per_isin[frag_measure] == 1\n",
    "# frag_per_isin.loc[nonfragmentation, \"fragmentation\"] = \"not fragmented\"\n",
    "# frag_per_isin.loc[~nonfragmentation, \"fragmentation\"] = \"fragmented\"\n",
    "\n",
    "# Option 2: by quartiles\n",
    "lower_quartile = frag_per_isin[frag_measure].quantile(0.25)\n",
    "median = frag_per_isin[frag_measure].median()\n",
    "upper_quartile = frag_per_isin[frag_measure].quantile(0.75)\n",
    "conditions = {\n",
    "    \"Q1\": frag_per_isin[frag_measure] < lower_quartile,\n",
    "    \"Q2\": (lower_quartile <= frag_per_isin[frag_measure]) & (frag_per_isin[frag_measure] < median),\n",
    "    \"Q3\": (median <= frag_per_isin[frag_measure]) & (frag_per_isin[frag_measure] < upper_quartile),\n",
    "    \"Q4\": upper_quartile <= frag_per_isin[frag_measure],\n",
    "}\n",
    "for fragmentation, condition in conditions.items():\n",
    "    frag_per_isin.loc[condition, \"fragmentation\"] = fragmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frag_per_isin[\"fragmentation\"].value_counts()\n",
    "\n",
    "# left join to stats\n",
    "stats = stats.join(frag_per_isin[\"fragmentation\"], on=\"isin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing those isin's that did not have 375 observations\n",
    "num_dates = stats.reset_index().groupby([\"fragmentation\", \"isin\"])[\"date\"].nunique()\n",
    "num_dates[num_dates != 375]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = stats.index.get_level_values(\"date\") < pd.Timestamp(\"2019-07-01\")\n",
    "num_stocks = stats.reset_index().groupby([\"fragmentation\"])[[\"isin\"]].nunique()  # .describe()\n",
    "print(f\"Total number of stocks {num_stocks['isin'].sum()}\")\n",
    "num_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember: groups can change over time, that's why there are more stocks than total above\n",
    "stats.reset_index().groupby([\"group\", \"fragmentation\"])[[\"isin\"]].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stats.reset_index().groupby([\"fragmentation\", \"turnover_category\", \"group\"])[[\"isin\"]].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[condition].reset_index().groupby([\"fragmentation\"])[[frag_measure]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.reset_index().groupby([\"after_nonequivalence\"])[[\"isin\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Cap variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[\"market_cap\"] = stats[\"shares_outstanding\"] * stats[\"price_close\"]\n",
    "market_cap_average_log = np.log(stats.groupby(\"isin\")[\"market_cap\"].mean())\n",
    "market_cap_average_log.name = \"market_cap_average_log\"\n",
    "stats = stats.join(market_cap_average_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(stats.reset_index().groupby([\"fragmentation\"])[[\"market_cap_average_log\"]].describe()).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragmentation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = list()\n",
    "for measure in (\"market_share\", \"lit_frag\", \"market_cap\", \"turnover\"):\n",
    "    descriptive = stats.reset_index().groupby([\"fragmentation\"])[[measure]].describe()\n",
    "    if measure == \"market_cap\":\n",
    "        descriptive /= 1e6\n",
    "        descriptive = descriptive.applymap(\"{:.0f}\".format)\n",
    "    elif measure == \"turnover\":\n",
    "        descriptive /= 1e6\n",
    "        descriptive = descriptive.applymap(\"{:.1f}\".format)\n",
    "    else:\n",
    "        descriptive = descriptive.applymap(\"{:.2f}\".format)\n",
    "        \n",
    "    descriptive = descriptive.loc[:, pd.IndexSlice[: , [\"mean\", \"50%\", \"std\"]]]\n",
    "    table.append(descriptive)\n",
    "    \n",
    "table = pd.concat(table, axis=1)\n",
    "table.rename(\n",
    "    columns={\n",
    "        \"market_share\": \"SIX market share\",\n",
    "        \"lit_frag\": \"LitFrag\",\n",
    "        \"market_cap\": \"Market Cap\",\n",
    "        \"turnover\": \"Turnover\",\n",
    "        \"mean\": \"Mean\",\n",
    "        \"std\": \"StDev\",\n",
    "        \"50%\": \"Median\"\n",
    "    }, \n",
    "    inplace=True,\n",
    ")\n",
    "table = table.T.reindex([\"Mean\", \"Median\", \"StDev\"], level=1).T\n",
    "\n",
    "num_stocks = stats.reset_index().groupby(\"fragmentation\")[\"isin\"].nunique()\n",
    "num_stocks = num_stocks.rename(\"Num stocks\").to_frame()\n",
    "num_stocks.columns = pd.MultiIndex.from_product([num_stocks.columns, ['']])\n",
    "table = table.join(num_stocks)\n",
    "\n",
    "for idx in range(4):\n",
    "    idx += 1\n",
    "    table.loc[f\"Q{idx}\", \"Fragmentation\"] = f\"Quartile {idx}\"\n",
    "table.set_index(\"Fragmentation\", inplace=True)\n",
    "\n",
    "table = table[[\"Num stocks\", \"SIX market share\", \"LitFrag\", \"Turnover\", \"Market Cap\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time variables & dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stats.loc[stats[\"fragmentation\"].isin([\"3_little_fragmented\", \"4_not_fragmented\"]), \"frag_dummy\"] = 0\n",
    "# stats[\"frag_dummy\"].fillna(value=1, inplace=True)\n",
    "# stats[\"frag_dummy\"] = stats[\"frag_dummy\"].astype(int)\n",
    "# stats.reset_index().groupby([\"frag_dummy\"])[[\"isin\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats[stats[\"frag_dummy\"] == 1].index.get_level_values(\"isin\").unique().to_frame().reset_index(drop=True).to_csv(\"frag_isins.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = stats.index.get_level_values(\"date\")\n",
    "stats.loc[7 <= dates.month, \"half_year\"] = \"H2\"\n",
    "stats[\"half_year\"].fillna(value=\"H1\", inplace=True)\n",
    "stats[\"semester\"] = dates.year.astype(\"str\") + \"_\" + stats[\"half_year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[\"dummy_2019\"] = dates.year == 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate daily returns & Amihud 2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.sort_index(inplace=True)\n",
    "\n",
    "stats[\"abs_simple_returns\"] = np.abs(stats[\"price_close\"] / stats[\"price_close\"].groupby(\"isin\").shift(1) - 1)\n",
    "stats[\"amihud\"] = stats[\"abs_simple_returns\"] / stats[\"turnover\"] * 1e9  # _simple_simple\n",
    "\n",
    "stats[[\"amihud\", \"semester\", \"fragmentation\"]].groupby([\"fragmentation\", \"semester\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot single measure for a quartile\n",
    "measure = \"eff_rel_spread_bps_weighted\"\n",
    "plot_data = stats.loc[stats[\"fragmentation\"] == \"Q4\", measure].reset_index().dropna()\n",
    "# px.scatter(plot_data, x=\"date\", y=measure, color=\"isin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isin = \"CH0012549785\"\n",
    "\n",
    "# measures = [\"price_mean\", \"price_close\", \"price_log\", \"price_reciprocal\"]\n",
    "measures = [\"quoted_rel_spread_bps_time_weighted\", \"eff_rel_spread_bps_weighted\", \"min_tick_size\"]\n",
    "# measures = [\"market_cap\", \"market_cap_average_log\", \"price_close\", \"shares_outstanding\"]\n",
    "\n",
    "plot_data = stats.loc[isin, measures]\n",
    "plot_data = plot_data.stack().reset_index().rename(columns={\"level_1\": \"measure\", 0: \"value\"})\n",
    "# px.scatter(plot_data, x=\"date\", y=\"value\", color=\"measure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_panel_regression(\n",
    "        data: pd.DataFrame, \n",
    "        measures: list,\n",
    "        control_variables: list,\n",
    "        entity_effects: bool,\n",
    "        time_effects: bool\n",
    "    ):\n",
    "    detailed_results = OrderedDict()\n",
    "\n",
    "    for idx, measure in enumerate(measures):\n",
    "\n",
    "        if measure.startswith((\"time\", \"depth\", \"num\", \"message_counts\", \"value\")) and not measure.endswith(\"percent\"):\n",
    "            dependent = np.log(data[measure])\n",
    "#             measure = measure + \"_log\"\n",
    "        else:\n",
    "            dependent = data[measure]\n",
    "            \n",
    "        if measure == \"amihud\":\n",
    "            control_variables = [var for var in exog_vars if var not in [\"log_turnover\", \"RV_slow\"]]\n",
    "            \n",
    "        elif measure == \"RV_slow\" or measure == \"VSMI\":\n",
    "            control_variables = [var for var in exog_vars if var not in [\"VSMI\", \"RV_slow\"]]\n",
    "        \n",
    "        elif measure in exog_vars:\n",
    "            control_variables = [var for var in exog_vars if var != measure]\n",
    "        \n",
    "        else:\n",
    "            control_variables = exog_vars\n",
    "        \n",
    "        exogenous = sm.add_constant(data[control_variables])\n",
    "\n",
    "        model = linearmodels.PanelOLS(dependent=dependent,\n",
    "                                      exog=exogenous,\n",
    "                                      entity_effects=entity_effects,\n",
    "                                      time_effects=time_effects,\n",
    "                                     )\n",
    "        try:\n",
    "            result = model.fit(cov_type='clustered',\n",
    "                               cluster_entity=True,\n",
    "                               cluster_time=True,\n",
    "                              )\n",
    "        except Exception as exception:\n",
    "            print(measure)\n",
    "            print(exception)\n",
    "            continue\n",
    "\n",
    "        # store the result\n",
    "        detailed_results[measure] = result\n",
    "        \n",
    "    return detailed_results\n",
    "    \n",
    "\n",
    "def deep_dive_coef(detailed_results, variable: str):\n",
    "\n",
    "    coef_results = pd.DataFrame(columns=[\"param\", \"lower\", \"upper\", \"tstat\", \"pvalue\"])  # , \"lower\", \"upper\"\n",
    "        \n",
    "    for measure, result in detailed_results.items():\n",
    "        param = result.params[variable]\n",
    "        lower, upper = result.conf_int().loc[variable]\n",
    "        tstat = result.tstats[variable]\n",
    "        pvalue = result.pvalues[variable]\n",
    "        coef_results.loc[measure] = (param, lower, upper, tstat, pvalue)  # , lower, upper\n",
    "    \n",
    "    return coef_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ols(data, measures, exog_vars):\n",
    "\n",
    "    detailed_results = OrderedDict()\n",
    "    \n",
    "    for idx, measure in enumerate(measures):\n",
    "        \n",
    "        if measure == \"amihud\":\n",
    "            control_variables = [var for var in exog_vars if var not in [\"log_turnover\", \"RV_slow\"]]\n",
    "            \n",
    "        elif measure == \"RV_slow\" or measure == \"VSMI\":\n",
    "            control_variables = [var for var in exog_vars if var not in [\"VSMI\", \"RV_slow\"]]\n",
    "        \n",
    "        elif measure in exog_vars:\n",
    "            control_variables = [var for var in exog_vars if var != measure]\n",
    "        \n",
    "        else:\n",
    "            control_variables = exog_vars\n",
    "        \n",
    "        exog = sm.add_constant(data[control_variables])\n",
    "\n",
    "        if measure.startswith((\"time\", \"depth\", \"num\", \"message_counts\", \"value\")) and not measure.endswith(\"percent\"):\n",
    "            endog = np.log(data[measure])\n",
    "        else:\n",
    "            endog = data[measure]\n",
    "\n",
    "        model = linearmodels.PooledOLS(endog, exog)\n",
    "        result = model.fit(\n",
    "            cov_type='clustered',\n",
    "            cluster_entity=True,\n",
    "            cluster_time=True,\n",
    "        )\n",
    "\n",
    "        # store the result\n",
    "        detailed_results[measure] = result\n",
    "        \n",
    "    return detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_lower_than(pvalue):\n",
    "    if pvalue < 0.01:\n",
    "        color = \"navajowhite\"  # \"darkgrey\"\n",
    "#             output = \"{:.3f} *\".format(value)\n",
    "    elif pvalue < 0.05:\n",
    "        color = \"blanchedalmond\"  # \"silver\"\n",
    "    elif pvalue < 0.1:\n",
    "        color = \"cornsilk\"  # \"gainsboro\"\n",
    "    else:\n",
    "        color = None\n",
    "    return f\"background-color: {color}\"\n",
    "\n",
    "def highlight_significance(data, pvalues):\n",
    "    background_colors = pvalues.applymap(highlight_lower_than)\n",
    "    return background_colors\n",
    "\n",
    "def font_color(value):\n",
    "    color = 'red' if value < 0 else 'black'\n",
    "    return f\"color: {color}\"\n",
    "\n",
    "def display_results(combined_results):\n",
    "    \n",
    "    params = combined_results[\"param\"]\n",
    "    pvalues = combined_results[\"pvalue\"]\n",
    "    \n",
    "    styled = params.round(3).style.applymap(font_color).apply(highlight_significance, pvalues=pvalues, axis=None)\n",
    "\n",
    "    return styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_to_significance(pvalue):\n",
    "    if pvalue < 0.01:\n",
    "        return \"***\"\n",
    "    elif pvalue < 0.05:\n",
    "        return \"**\"\n",
    "    elif pvalue < 0.05:\n",
    "        return \"*\"\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def format_pvalues(series):\n",
    "    return series.apply(lambda val: val.apply(convert_to_significance))\n",
    "\n",
    "def format_stars(table, precision=3):\n",
    "    \n",
    "    lower = table[[\"lower\"]].round(precision).astype(str)\n",
    "    lower.columns = lower.columns.droplevel()\n",
    "    upper = table[[\"upper\"]].round(precision).astype(str)\n",
    "    upper.columns = upper.columns.droplevel()\n",
    "    confidence = \"[\" + lower + \",  \" + upper + \"]\"\n",
    "    confidence.columns = pd.MultiIndex.from_product([['conf'], confidence.columns])\n",
    "    \n",
    "    format_num = \"{:.\" + f\"{precision}\" + \"f}\"\n",
    "    params = table[\"param\"].applymap(lambda num: format_num.format(num))\n",
    "    pvalues = table[\"pvalue\"]\n",
    "    tstats = table[[\"tstat\"]].applymap(lambda num: \"(\" + format_num.format(num) + \")\")\n",
    "    \n",
    "    params = pvalues.applymap(convert_to_significance) + params\n",
    "    params.columns = pd.MultiIndex.from_product([['coef'], params.columns])\n",
    "    \n",
    "    formatted = pd.concat([params, tstats, confidence])\n",
    "    formatted.columns.rename(\"coef_type\", level=0, inplace=True)\n",
    "    formatted = formatted.stack(\"coef_type\")\n",
    "    formatted.columns.rename(\"frag_quartile\", inplace=True)\n",
    "\n",
    "    formatted = formatted.reindex(sorted(formatted.columns), axis=1)\n",
    "    formatted.sort_values(by=[\"measure\", \"coef_type\"], ascending=True, inplace=True)\n",
    "    \n",
    "    return formatted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liquidity_measures = [\n",
    "    'quoted_rel_spread_bps_time_weighted',\n",
    "    'eff_rel_spread_bps_weighted',\n",
    "    'depth_time_weighted_average',\n",
    "]\n",
    "amihud_turnover_measures = [\"log_turnover\", \"RV_slow\", \"amihud\"]\n",
    "\n",
    "counts_measures = measures = [\n",
    "    'AT_proxy',\n",
    "    'num_orders_aggr',\n",
    "    'num_orders_passive',\n",
    "    'num_orders_deleted',\n",
    "    'num_orders_filled',\n",
    "    'value_entered_mean',\n",
    "    'value_entered_median',\n",
    "    'value_entered_total',\n",
    "    'value_filled_total',\n",
    "]\n",
    "\n",
    "\n",
    "all_measures = liquidity_measures + amihud_turnover_measures + counts_measures\n",
    "measures = all_measures\n",
    "\n",
    "control_vars = [\n",
    "#     \"RV_slow\",\n",
    "    \"VSMI\",  # Riordan & Storkenmaier 2012 JFM, p.427, quotes Hendershott & Moulton 2011 JFM, p.583\n",
    "    \"min_tick_size\",\n",
    "    \"price_log\",\n",
    "]\n",
    "\n",
    "\n",
    "explaining_variable = \"after_nonequivalence\"  # \"dummy_2019\"\n",
    "\n",
    "exog_vars = [explaining_variable] + control_vars\n",
    "exog_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detailed_results = dict()\n",
    "coef_results = dict()\n",
    "\n",
    "conditions = {\n",
    "    \"\": pd.Series(True, index=stats.index),  # all_\n",
    "#     \"2019_only_\": stats.index.get_level_values(\"date\").year == 2019,\n",
    "#     \"H2_only_\": stats[\"half_year\"] == \"H2\",\n",
    "#     \"before_\": stats.index.get_level_values(\"date\") < pd.Timestamp(\"2019-07-01\")\n",
    "}\n",
    "\n",
    "for condition_name, condition in conditions.items():\n",
    "    \n",
    "    subset = stats[condition]\n",
    "    \n",
    "#     # Full sample\n",
    "#     regression_name = f\"{condition_name}Full sample\"\n",
    "#     detailed_result = run_panel_regression(subset, measures, exog_vars, entity_effects=True, time_effects=False)\n",
    "#     detailed_results[regression_name] = detailed_result\n",
    "#     coef_result = deep_dive_coef(detailed_result, explaining_variable)\n",
    "#     coef_results[regression_name] = coef_result\n",
    "    \n",
    "    # Per fragmentation quartile\n",
    "    for frag_dummy, data in tqdm(subset.groupby(\"fragmentation\")):\n",
    "\n",
    "        regression_name = f\"{condition_name}{frag_dummy}\"\n",
    "        detailed_result = run_panel_regression(data, measures, exog_vars, entity_effects=True, time_effects=False)\n",
    "        detailed_results[regression_name] = detailed_result\n",
    "        coef_result = deep_dive_coef(detailed_result, explaining_variable)\n",
    "        coef_results[regression_name] = coef_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat(coef_results)\n",
    "combined.index.set_names([\"fragmentation\", \"measure\"], inplace=True)\n",
    "combined = combined.unstack(\"fragmentation\")\n",
    "combined.columns.set_names([\"coef_type\", \"fragmentation\"], inplace=True)\n",
    "combined = combined.reindex(combined.columns.sortlevel(level=\"fragmentation\")[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define here which variables we'd like to see\n",
    "subset = liquidity_measures + amihud_turnover_measures   # counts_measures / liquidity_measures / amihud_turnover_measures\n",
    "\n",
    "subset = combined.loc[subset].copy()\n",
    "export_this = format_stars(subset, precision=2)\n",
    "export_this.reset_index(\"coef_type\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_this[\"coef_type\"] = export_this[\"coef_type\"].astype(\"category\")\n",
    "export_this[\"coef_type\"] = export_this[\"coef_type\"].cat.reorder_categories([\"coef\", \"tstat\", \"conf\"], ordered=True)\n",
    "export_this = export_this.sort_values([\"measure\", \"coef_type\"]).drop(columns=\"coef_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_this.rename(\n",
    "    index={\n",
    "        \"quoted_rel_spread_bps_time_weighted\": \"QSpread\",\n",
    "        \"eff_rel_spread_bps_weighted\": \"ESpread\",\n",
    "        \"depth_time_weighted_average\": \"lnDepth\",\n",
    "        \"AT_proxy\": \"AT_proxy\",\n",
    "        \"num_orders_aggr\":\"Num aggressive Orders\",\n",
    "        \"num_orders_deleted\": \"Num deleted Orders\",\n",
    "        \"num_orders_filled\": \"Num filled Orders\",\n",
    "        \"num_orders_passive\": \"Num passive Orders\",\n",
    "        \"value_entered_total\": \"Log Volume Entered\",\n",
    "        \"value_filled_total\": \"Log Volume Filled\",\n",
    "    },\n",
    "    columns={col: \"Quartile \" + col[-1] for col in export_this.columns},\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(export_this.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "measure = measures[0]\n",
    "pprint(measures)\n",
    "print(f\"\\nSelected: {measure}\")\n",
    "samples = combined.columns.get_level_values(\"fragmentation\").unique().tolist()\n",
    "regr_table = linearmodels.panel.compare([detailed_results.get(sample).get(measure) for sample in samples], precision=\"pvalues\")\n",
    "regr_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS with stock-level controls\n",
    "Riordan & Storkenmeier 2012, Hendershott & Moulton 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"market_cap_average_log\" not in control_vars:\n",
    "    control_vars += [\"market_cap_average_log\"]\n",
    "\n",
    "exog_vars = [explaining_variable] + control_vars\n",
    "\n",
    "exog_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_results = dict()\n",
    "coef_results = dict()\n",
    "\n",
    "conditions = {\n",
    "    \"\": pd.Series(True, index=stats.index),  # all_\n",
    "#     \"2019_only_\": stats.index.get_level_values(\"date\").year == 2019,\n",
    "#     \"H2_only_\": stats[\"half_year\"] == \"H2\",\n",
    "#     \"before\": stats.index.get_level_values(\"date\") < pd.Timestamp(\"2019-07-01\")\n",
    "}\n",
    "\n",
    "for condition_name, condition in tqdm(conditions.items()):\n",
    "    \n",
    "    subset = stats[condition]\n",
    "    \n",
    "#     # Full sample\n",
    "#     regression_name = f\"{condition_name}Full sample\"\n",
    "#     detailed_result = run_panel_regression(subset, measures, exog_vars, entity_effects=True, time_effects=False)\n",
    "#     detailed_results[regression_name] = detailed_result\n",
    "#     coef_result = deep_dive_coef(detailed_result, explaining_variable[0])\n",
    "#     coef_results[regression_name] = coef_result\n",
    "    \n",
    "    # Per fragmentation quartile\n",
    "    for frag_dummy, data in subset.groupby(\"fragmentation\"):\n",
    "\n",
    "        regression_name = f\"{condition_name}{frag_dummy}\"\n",
    "        detailed_result = run_ols(data, measures, exog_vars)\n",
    "        detailed_results[regression_name] = detailed_result\n",
    "        coef_result = deep_dive_coef(detailed_result, explaining_variable)\n",
    "        coef_results[regression_name] = coef_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat(coef_results)\n",
    "combined.index.set_names([\"fragmentation\", \"measure\"], inplace=True)\n",
    "combined = combined.unstack(\"fragmentation\")\n",
    "combined.columns.set_names([\"coef_type\", \"fragmentation\"], inplace=True)\n",
    "combined = combined.reindex(combined.columns.sortlevel(level=\"fragmentation\")[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_this = format_stars(combined, precision=3)\n",
    "# print(export_this.to_latex(sparsify=True))\n",
    "export_this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_results(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pprint(measures)\n",
    "measure = measures[0]\n",
    "print(f\"\\nSelected: {measure}\")\n",
    "samples = combined.columns.get_level_values(\"fragmentation\").unique().tolist()\n",
    "linearmodels.panel.compare([detailed_results.get(sample).get(measure) for sample in samples], precision=\"pvalues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes of averages\n",
    "\n",
    "similar to Riordan & Storkenmaier JFM 2012 p.426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_subset = all_measures + [\"turnover\"]\n",
    "frag_measure = \"fragmentation\"\n",
    "\n",
    "averages = stats.groupby([\"after_nonequivalence\", frag_measure])[measures_subset].describe().sort_index(level=frag_measure)\n",
    "\n",
    "# transform CHF cols to CHF 1000\n",
    "depth_cols = [col for col in averages.columns if col[0].startswith(\"depth\") or col[0].startswith(\"turnover\")]\n",
    "averages[depth_cols] = averages[depth_cols] / 1000\n",
    "\n",
    "averages.columns = averages.columns.swaplevel()\n",
    "averages = averages[[\"mean\"]]  #, \"std\", \"50%\", \n",
    "averages.rename(columns={\"50%\": \"median\"}, inplace=True)\n",
    "\n",
    "averages.columns = averages.columns.swaplevel()\n",
    "averages = averages.unstack(\"fragmentation\").sort_index(axis=1).round(3)\n",
    "\n",
    "# averages.loc[\"diff\"] = averages.diff().loc[True]\n",
    "averages.loc[\"relative change\"] = (averages.loc[True] - averages.loc[False]) / np.abs(averages.loc[False])\n",
    "\n",
    "averages = averages.stack(\"fragmentation\").sort_index(level=\"fragmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = averages.round(2)\n",
    "averages.reset_index(\"after_nonequivalence\", inplace=True)\n",
    "averages[\"after_nonequivalence\"] = averages[\"after_nonequivalence\"].replace({True: \"post non-eq\", False: \"pre non-eq\"})\n",
    "averages[\"after_nonequivalence\"] = averages[\"after_nonequivalence\"].astype(\"category\")\n",
    "cat_order = [\"pre non-eq\", \"post non-eq\", \"relative change\"]\n",
    "averages[\"after_nonequivalence\"] = averages[\"after_nonequivalence\"].cat.reorder_categories(cat_order, ordered=True)\n",
    "averages.set_index(\"after_nonequivalence\", append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages.columns = averages.columns.swaplevel().droplevel()\n",
    "\n",
    "averages = averages.stack().unstack(\"fragmentation\")\n",
    "averages.index = averages.index.swaplevel()\n",
    "averages.sort_index(inplace=True)\n",
    "averages.style.format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def font_negative_positive(value):\n",
    "    if value < 0:\n",
    "        color = 'red'\n",
    "    elif value > 0:\n",
    "        color = \"limegreen\"\n",
    "    else:\n",
    "        color = \"black\"\n",
    "    return f\"color: {color}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "averages_table = averages.style.format(\"{:.2f}\")\n",
    "# averages_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = averages.loc[idx[:, \"relative change\"], :]\n",
    "diffs.style.applymap(font_negative_positive).format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(diffs.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changes of sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of messages\n",
    "stats[\"year\"] = stats.index.get_level_values(\"date\").year\n",
    "\n",
    "measures_subset = [\"turnover\", \"num_orders_passive\"]\n",
    "\n",
    "frag_measure = \"fragmentation\"\n",
    "\n",
    "sums = stats.groupby([\"date\", frag_measure])[measures_subset].sum().sort_index(level=frag_measure)\n",
    "\n",
    "sums.rename(columns={\"num_orders_aggr\": \"num_orders_aggr\"}, inplace=True)\n",
    "sums = sums.stack().to_frame().reset_index()\n",
    "sums.rename(columns={0: \"value\", \"level_2\": \"measure\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(sums, x=\"date\", y=\"value\", color=\"fragmentation\", facet_row=\"measure\")\n",
    "fig.update_yaxes(matches=None)  # free y-axis scale\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sums.loc[\"diff\"] = sums.diff().loc[True]\n",
    "\n",
    "# sums = sums.stack(\"fragmentation\").sort_index(level=\"fragmentation\")\n",
    "# relative_difference = sums.loc[True] / sums.loc[False] - 1\n",
    "\n",
    "# sums = sums.T / 1e6  # in millions\n",
    "# sums.drop([\"AT_proxy\"], inplace=True)\n",
    "sums = sums.stack().stack().to_frame().reset_index().rename(columns={0: \"# mn\", \"level_3\": \"measure\", \"fragmentation\": \"Fragmentation quartile\"})\n",
    "\n",
    "sums[\"semester\"] = sums[\"year\"].astype(str) + \"_\" + sums[\"half_year\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = \"semester\"\n",
    "facet = \"measure\"\n",
    "xaxis = \"Fragmentation quartile\"\n",
    "plot_data = sums.sort_values([color, facet], inplace=True)\n",
    "fig = px.bar(\n",
    "    sums,\n",
    "    x=xaxis,\n",
    "    y=\"# mn\",\n",
    "    color=color,\n",
    "    facet_row=facet,\n",
    "    height=881,\n",
    "    text=\"# mn\",\n",
    "    template='plotly_white',\n",
    "    color_discrete_sequence=[\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\",  \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\"],  # px.colors.qualitative.Prism,\n",
    ")\n",
    "fig.update_yaxes(matches=None)  # free y-axis scale\n",
    "fig.update_layout(barmode='group', xaxis={'categoryorder':'category ascending'})\n",
    "fig.update_traces(textposition='outside', texttemplate=\"%{value:.2f}\")\n",
    "fig.for_each_trace(\n",
    "    lambda trace: trace.update(\n",
    "        name=trace.name.replace(f\"{color}=\", \"\"),\n",
    "    )\n",
    ")\n",
    "for annotation in fig.layout.annotations:\n",
    "    annotation.text = annotation.text.split(\"=\")[1]\n",
    "# fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats[\"num_orders_filled\"].sum() / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-sample test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ksample_test(series, measure):\n",
    "    series = series[[measure, \"after_nonequivalence\"]]\n",
    "    before = series.loc[~series.after_nonequivalence.astype(bool), measure]\n",
    "    after = series.loc[series.after_nonequivalence.astype(bool), measure]\n",
    "    \n",
    "    # if we don't find data before or after non-equivalence\n",
    "    if (all(before.isna()) == True) and (all(after.isna()) == True):\n",
    "        return pd.Series({\"statistic\": np.nan, \"significance_level\": np.nan})\n",
    "    else:\n",
    "        try:\n",
    "            result = anderson_ksamp([before.values, after.values])\n",
    "        except:\n",
    "            print(all(before.isna()))\n",
    "            print(after.isna().all())\n",
    "        return pd.Series({\"statistic\": result.statistic, \"significance_level\": result.significance_level})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "measure = \"time_to_removal_mean\"\n",
    "results = stats.groupby(\"isin\").apply(lambda this_stock: calculate_ksample_test(this_stock, measure))\n",
    "results.dropna(how=\"all\", inplace=True)\n",
    "results[\"significant 2.5%\"] = results[\"significance_level\"] < 0.025\n",
    "# results = results.join(frag_before_nonequivalence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.join(stats[\"fragmentation\"].reset_index(level=\"date\"), on=\"isin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby(\"fragmentation\")[\"significant 2.5%\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06616904709341fa9093bad777df4333": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_e629328220eb4f73bcc780e01efb3c78",
       "max": 1,
       "style": "IPY_MODEL_44d4719756d248989e65f0cc1cf34dbc",
       "value": 1
      }
     },
     "06ce0ad7e0ec4013847d923a503ed3b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0a7ec9341f464eff9a70dd6cd77d8942": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_5afe4ec28cf044e6adc81f77aafadb05",
       "max": 1,
       "style": "IPY_MODEL_81380526f71845b5a720224b330cd253",
       "value": 1
      }
     },
     "17decb944adb45f2a200e56196b096ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_06ce0ad7e0ec4013847d923a503ed3b3",
       "style": "IPY_MODEL_236a26845bdd4c87b05c3a28df910129",
       "value": " 1/1 [04:18&lt;00:00, 258.37s/it]"
      }
     },
     "236a26845bdd4c87b05c3a28df910129": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "44d4719756d248989e65f0cc1cf34dbc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "5afe4ec28cf044e6adc81f77aafadb05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "65cc96fddcd849f2957e00bd9f39731b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "765bb1d778fb444994d1de5a6227bc77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9420f21da4b344f3b5edbc61884abad2",
       "style": "IPY_MODEL_e0643de2c3954bee8ef1b0590cc50e60",
       "value": " 1/1 [09:50&lt;00:00, 590.80s/it]"
      }
     },
     "8134572ed238473eb209e1ad95f5ed7c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "81380526f71845b5a720224b330cd253": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "9420f21da4b344f3b5edbc61884abad2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a63eca84767d4a0098021d4312c5b8e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_06616904709341fa9093bad777df4333",
        "IPY_MODEL_17decb944adb45f2a200e56196b096ff"
       ],
       "layout": "IPY_MODEL_65cc96fddcd849f2957e00bd9f39731b"
      }
     },
     "ccd0a50572d94928b61ae2c625d717ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_0a7ec9341f464eff9a70dd6cd77d8942",
        "IPY_MODEL_765bb1d778fb444994d1de5a6227bc77"
       ],
       "layout": "IPY_MODEL_8134572ed238473eb209e1ad95f5ed7c"
      }
     },
     "e0643de2c3954bee8ef1b0590cc50e60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e629328220eb4f73bcc780e01efb3c78": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
